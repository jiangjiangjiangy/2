{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install xgboost\n","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('train_set.csv', sep='\\t')\ntest = pd.read_csv('test_a.csv', sep='\\t')","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"train_text = train['text']\ntest_text = test['text']\nall_text = pd.concat([train_text, test_text])","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"%%time\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=10000)\n\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)\ntrain_word_features","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"X_train = train_word_features\ny_train = train['label']\n\n# 可以改变输入维度\nx_train_, x_valid_, y_train_, y_valid_ = train_test_split(X_train, y_train, test_size=0.2)\nX_test = test_word_features","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"%%time\nclf = LogisticRegression(C=4, n_jobs=16)\nclf.fit(x_train_, y_train_)\n\ny_pred = clf.predict(x_valid_)\ntrain_scores = clf.score(x_train_, y_train_)\nprint(train_scores, f1_score(y_pred, y_valid_, average='macro'))","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"class XGB():\n    \n    def __init__(self, X_df, y_df):\n        self.X = X_df\n        self.y = y_df\n       \n    def train(self, param):\n        self.model = XGBClassifier(**param)\n        self.model.fit(self.X, self.y, eval_set=[(self.X, self.y)],\n                       eval_metric=['mlogloss'],\n                       early_stopping_rounds=10,  # 连续N次分值不再优化则提前停止\n                       verbose=False\n                      )\n        \n#         mode evaluation\n        train_result, train_proba = self.model.predict(self.X), self.model.predict_proba(self.X)\n        train_acc = accuracy_score(self.y, train_result)\n        train_auc = f1_score(self.y, train_proba, average='macro')\n        \n        print(\"Train acc: %.2f%% Train auc: %.2f\" % (train_acc*100.0, train_auc))\n        \n    def test(self, X_test, y_test):\n        result, proba = self.model.predict(X_test), self.model.predict_proba(X_test)\n        acc = accuracy_score(y_test, result)\n        f1 = f1_score(y_test, proba, average='macro')\n        \n        print(\"acc: %.2f%% F1_score: %.2f%%\" % (acc*100.0, f1))\n    \n    def grid(self, param_grid):\n        self.param_grid = param_grid\n        xgb_model = XGBClassifier(nthread=20)\n        clf = GridSearchCV(xgb_model, self.param_grid, scoring='f1_macro', cv=2, verbose=1)\n        clf.fit(self.X, self.y)\n        print(\"Best score: %f using parms: %s\" % (clf.best_score_, clf.best_params_))\n        return clf.best_params_, clf.best_score_\n\n    ","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"x_train_, x_valid_, y_train_, y_valid_ = train_test_split(X_train[:, :300], y_train, test_size=0.2, shuffle=True, random_state=42)\nX_test = test_word_features[:,:300]","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\n# 假设您已经训练了一个模型 model\n# model = xgb.train(param, dtrain)\n\n# 预测验证集的类别索引\ny_pred = model.predict(x_valid_)\n\n# 使用类别索引获取类别概率\nprobs = model.predict_proba(x_valid_)\n\n# probs 是一个 NumPy 数组，其中每一行对应于一个样本，每一列对应于一个类别\n# 现在你可以使用 probs 来计算混淆矩阵、计算准确率等\n","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"%%time\nparam = {'learning_rate': 0.05,         #  (xgb’s “eta”)\n              'objective': 'multi:softmax', \n              'n_jobs': 16,\n              'n_estimators': 300,           # 树的个数\n              'max_depth': 10,               \n              'gamma': 0.5,                  # 惩罚项中叶子结点个数前的参数，Increasing this value will make model more conservative.\n              'reg_alpha': 0,               # L1 regularization term on weights.Increasing this value will make model more conservative.\n              'reg_lambda': 2,              # L2 regularization term on weights.Increasing this value will make model more conservative.\n              'min_child_weight' : 1,      # 叶子节点最小权重\n              'subsample':0.8,             # 随机选择80%样本建立决策树\n              'random_state':1           # 随机数\n             }\nmodel = XGB(x_train_, y_train_)\nmodel.train(param)\nmodel.test(x_valid_, y_valid_)","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"final_model = XGB(X_train, y_train)\nfinal_model.train(param)\n\nsubmission = pd.read_csv('./datalab/72510/test_a_sample_submit.csv')\npreds = final_model.model.predict(X_test)\nsubmission['label'] = preds\nsubmission.to_csv('./xgb_submission.csv', index=False)","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"","metadata":{},"outputs":[]}]}